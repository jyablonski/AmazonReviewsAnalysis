# loading packages and adjusting keywords ----
# The Data - https://registry.opendata.aws/amazon-reviews/
# Data from 12-11-2012 to 8-31-2015

library(tidytext)
library(tidyverse)
library(extrafont)
library(magick)
library(scales)
library(lubridate)
library(igraph)
library(ggraph)
library(widyr)
library(wordcloud)
library(reshape2)
library(GGally)
library(fpp3)
library(sugrrants)
library(textrecipes)
library(knitr)
library(vip)
library(skimr)
library(tidymodels)

dateVariable <- Sys.Date()
dateVariable <-  format(dateVariable, format = "%B %d, %Y")

# custom theme
theme_jacob <- function () { 
  theme_minimal(base_size=9, base_family="Gill Sans MT") %+replace% 
    theme(
      panel.grid.minor = element_blank(),
      plot.background = element_rect(fill = 'floralwhite', color = 'floralwhite')
    )
}


# loading data in and formatting it ----
df <- read_csv('Amazon Video Game Review Data.csv ')

df2 <- df %>%
  select(customer_id, product_title, star_rating, helpful_votes, total_votes,
         verified_purchase, review_headline, review_body, review_date) %>%
  rename(CustID = customer_id, Title = product_title, Rating = star_rating,
         HelpVotes = helpful_votes, TotalVotes = total_votes, 
         VerifiedPurchase = verified_purchase, Headline = review_headline,
         Review = review_body, Date = review_date) %>%
  filter(!is.na(Date))
df2$Date <- mdy(df2$Date)
df2 <- as.data.frame(df2)
str(df2)

skim(df2)
# Top 10 Customers
df2 %>%
  select(CustID, Review) %>%
  group_by(CustID) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count)) %>%
  top_n(10)
  
df2 %>%
  select(Title) %>%
  group_by(Title) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count)) %>%
  top_n(10)

# building ratings table + graph
df2ratings <- df2 %>%
  count(Rating) %>%
  mutate(Pct = (n / sum(n)) * 100) %>%
  mutate('Percent of Total' = round(Pct, 2)) %>%
  select(-Pct)

ggplot(df2, aes(x = Rating)) +
  geom_histogram() +
  scale_y_continuous(labels = comma) +
  labs(title = 'Amazon Rating Counts') +
  theme_jacob()
ggsave("amazonratingcounts.png", width = 9, height = 4)

head(df2$Review)

# specific product titles
df2GTAV <- df2 %>%
  filter(Title == 'Grand Theft Auto V')
ggplot(df2GTAV, aes(x = Rating)) +
  geom_histogram() +
  scale_y_continuous(labels = comma) +
  labs(title = 'GTA V Ratings') +
  theme_jacob()

df2PS3 <- df2 %>%
  filter(Title == 'PlayStation 4 500GB Console [Old Model]') %>%
  group_by(Date) %>%
  summarise(Count = n())

ggplot(df2PS3, aes(x = Rating)) +
  geom_histogram() +
  scale_y_continuous(labels = comma) +
  labs(title = 'PS3 Ratings') +
  theme_jacob()


# list of every columns na values.
colSums(is.na(df2))

ggplot(df2, aes(Date)) +
  geom_bar() +
  scale_y_continuous(labels = comma) +
  scale_x_date(date_breaks = "4 months", date_labels = "%b %Y") +
  theme_jacob()

df2Feb <- df2 %>%
  filter(month(Date) == '2')

df2Months <- df2 %>%
  mutate(Month = yearmonth(Date)) %>%
  group_by(Month) %>%
  summarise(Count = n())

df2days <- df2 %>%
  group_by(Date) %>%
  summarise(Count = n()) %>%
  mutate(impDate = ifelse(Count %in% tail(sort(Count), 10), 'Important Date', " "))

# time series graph ----
ggplot(df2days, aes(Date, Count, fill = impDate)) +
  geom_bar(stat = 'identity', width = 1.2) +
  scale_y_continuous(labels = comma) +
  scale_fill_manual(values = c("#969696", "#de2d26")) +
  scale_x_date(date_breaks = "4 months", date_labels = "%b %Y") +
  annotate(geom = 'text', y = 3400, x = as.Date("2015-03-20"),
           label = "Red Lines Indicate Top 10 Most Popular Days", family = "Gill Sans MT", size = 2.25, hjust = 0.2) +
  annotate(geom = 'text', y = 3250, x = as.Date("2015-03-20"),
           label = "(The weeks following Christmas Day)", family = "Gill Sans MT", size = 2.25, hjust = 0.2) +
  labs(title = "Count of Amazon Video Game Reviews", 
       subtitle = "Data Ranging from Dec. 2012 - Aug. 2015",
       x = "Date", 
       y = "Total Reviews", 
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = "")) +
  theme_jacob() +
  theme(plot.title = element_text(face = 'bold', hjust = .5), 
        plot.subtitle = element_text(hjust = .5), 
        legend.position = 'none') 
ggsave("amazonreviews.png", width = 9, height = 4)
str(df2days)




# Forecasting next 12 months of Number of Reviews ----
dfForecast <- as_tsibble(df2)
dfForecast <- df2Months %>%
  as_tsibble(index = Month)

autoplot(dfForecast)

# exponential smoothing - log cost to get rid of potential negative forecast (can't have a negative forecast for this.)
pfit <- dfForecast %>%
  model(ETS(log(Count) ~ error("M") + trend("Ad") + season("M")))
report(pfit)
abc <- augment(pfit) # fitted values & forecast horizon stuff
gg_tsresiduals(pfit) # checking residuals.  
accuracy(fc, dfForecast)
fc <- pfit %>%
  forecast(h = 12)


pfit %>%
  forecast(h = 12) %>%
  autoplot(dfForecast) +
  scale_x_date(date_breaks = "4 months", date_labels = "%b %Y") +
  theme_jacob() +
  xlab("Year") + ylab("Count") +
  ggtitle("Number of Monthly Amazon Reviews for Video Game Related Products") +
  labs(subtitle = 'Exponential Smoothing Forecast (Damped Trend + Seasonality)') +
  theme_jacob()
ggsave('amazonexponentialsmoothing.png', width = 9, height = 4)

# ARIMA
fit_arima <- dfForecast %>%
  model(ARIMA(Count)) #(1, 0, 0) (0, 1, 0) [12]
report(fit_arima)
gg_tsresiduals(fit_arima, lag_max = 6)
augment(fit_arima) %>%
  features(.resid, ljung_box, lag = 12, dof = 3) 

fit_arima %>%
  forecast(h = 12) %>%
  autoplot(dfForecast) +
  scale_x_date(date_breaks = "4 months", date_labels = "%b %Y") +
  xlab("Year") + ylab("Count") +
  ggtitle("Number of Monthly Amazon Reviews for Video Game Related Products") +
  labs(subtitle = 'Auto ARIMA Forecast') +
  theme_jacob()
ggsave('amazonautoarima.png', width = 9, height = 4)
  
# maybe do tidy text on ratings = 1 vs ratings = 5
# maybe model help ratings number of help votes vs number of words in the text????????
df2helpRatings <- df2 %>%
  filter(HelpVotes >= 200)

df2lowRatings <- df2 %>%
  filter(Rating == 1)

df2highRatings <- df2 %>%
  filter(Rating == 5)

# Tidy text stuff on Review Body ----
dfWords <- df2 %>%
  dplyr::select(Review) %>%
  unnest_tokens(word, Review)

# a lot of stop words in the top 15 
dfWords %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = "Count of unique words found in tweets") +
  theme_jacob()

# filter out the stop words (in, of, as, the etc.)
data(stop_words)
nrow(dfWords) # 60,081,053 words before

dfWords <- dfWords %>%
  anti_join(stop_words)
nrow(dfWords) # 22,019,080 words after

# top 25words graph - Picture 1
dfWords %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique Words",
       title = "Top 25 Most Popular Words",
       subtitle = 'Video Game Product Reviews on Amazon',
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = "")) +
  theme_jacob()
ggsave('amazontop25wordsReviews.png', width = 9, height = 4)


# bigram stuff
dfbigramz <- df2 %>%
  head(5000)
# looking at paired words in tweets - bigrams
bigram <- dfbigramz %>%
  dplyr::select(Review) %>%
  unnest_tokens(bigram, Review, token = "ngrams", n = 2)

bigrams_separated <- bigram  %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>%
  filter(word1 != 'br') %>%
  filter(word2 != 'br')

head(bigram_counts, 15)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Word Network Graph - Picture 2
bigram_counts %>%
  filter(n >= 20) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_node_point(color = "darkslategray4", size = 3) +
  geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
  labs(title = "Most Popular Word Pairs",
       subtitle = "Text Mining with Amazon Review Data",
       x = "", y = "") +
  theme_jacob()
ggsave('amazonwordpairs.png', width = 9, height = 4)

# filter out any noticeable sentiment word that is out of place here
bing_word_counts <- dfWords %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# Top 15 Graph for each Sentiment - Picture 3
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Top 15 Most Popular Words Tweeted by Each Sentiment",
       x = NULL, 
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = ""))  +
  coord_flip() +
  theme_jacob()
ggsave('amazontop15Bing.png', width = 9, height = 4)

dfWords %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 50))

dfWords %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "blue"),
                   max.words = 100) +
  theme_jacob()

# word Counts
word_counts <- dfWords %>%
  count(word, sort = TRUE) %>%
  mutate(total = sum(n))

# Frequency of word counts
freq_by_rank <- word_counts %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)
freq_by_rank
head(freq_by_rank, 6) # top 6 words include ~19 of all data
top6 <- sum(head(freq_by_rank$`term frequency`, 6)) * 100
cat("Top 6 Words represent ", top6, "% of the entire dataset", sep = "" )


# Headline Text Mining ----
dfWordsHeadline <- df2 %>%
  dplyr::select(Headline) %>%
  unnest_tokens(word, Headline)

# a lot of stop words in the top 15 
dfWordsHeadline %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = "Count of unique words found in tweets") +
  theme_jacob()

# filter out the stop words (in, of, as, the etc.)
data(stop_words)
nrow(dfWordsHeadline) # 4,160,561 words before

dfWordsHeadline <- dfWordsHeadline %>%
  anti_join(stop_words)
nrow(dfWordsHeadline) # 1,924,161 words after

# top 25words graph - Picture 1
dfWordsHeadline %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Top 25 Most Popular Words",
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = "")) +
  theme_jacob()


# filter out any noticeable sentiment word that is out of place here
bing_word_counts_headline <- dfWordsHeadline %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# Top 15 Graph for each Sentiment - Picture 3
bing_word_counts_headline %>%
  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Top 15 Most Popular Words Tweeted by Each Sentiment",
       x = NULL, 
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = ""))  +
  coord_flip() +
  theme_jacob()

# all 3 sentiments at once
afinn <- dfWordsHeadline %>%
  inner_join(get_sentiments('afinn')) %>%
  group_by(value) %>%
  summarise(Count = n()) %>%
  mutate(method = 'AFINN')

bing_and_nrc <- bind_rows(dfWordsHeadline %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          dfWordsHeadline %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

badwords <- dfWordsHeadline %>%
  inner_join(get_sentiments('afinn')) %>%
  filter(value == -5) %>%
  group_by(word) %>%
  summarise(Count = n())

afinnbad <- dfWordsHeadline %>%
  inner_join(get_sentiments('afinn')) %>%
  filter(value < 0)

afinngood <- dfWordsHeadline %>%
  inner_join(get_sentiments('afinn')) %>%
  filter(value > 0)

goodwords <- dfWordsHeadline %>%
  inner_join(get_sentiments('afinn')) %>%
  filter(value == 5) %>%
  group_by(word) %>%
  summarise(Count = n())


# top 25words graph - Picture 1
afinnbad %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique Words",
       title = "Top 25 Most Popular Words in Review Headlines",
       subtitle = 'AFINN Sentiment Analysis - Negative Words',
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = "")) +
  theme_jacob()
ggsave('amazonnegativewords.png', width = 9, height = 4)

# top 25words graph - Picture 1
afinngood %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique Words",
       title = "Top 25 Most Popular Words in Review Headlines",
       subtitle = 'AFINN Sentiment Analysis - Positive Words',
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = "")) +
  theme_jacob()
ggsave('amazonpositivewords.png', width = 9, height = 4)


# Lasso Machine Learning ----
# loading in data and unnesting it
reviews_parsed <- df2 %>%
  mutate(review_id = row_number(),
         general_rating = case_when(Rating > 3 ~ "good",
                            TRUE ~ "bad"))

words_per_review <- reviews_parsed %>%
  unnest_tokens(word, Review) %>%
  count(review_id, name = "total_words")

words_per_review %>%
  ggplot(aes(total_words)) +
  geom_histogram(fill = 'midnight blue', alpha = 0.8, color = 'black') +
  scale_x_continuous(limits = c(0, 250)) +
  labs(title = "Number of Words Distribution", 
       subtitle = "Amazon Review Data Ranging from Dec. 2012 - Aug. 2015",
       x = "Total Words", 
       y = "Count", 
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = "")) +
  theme_jacob()
ggsave('AmazonHistogramWords.png', width = 9, height = 4)


reviews_parsed <- as_tibble(reviews_parsed) %>%
  slice(1:100000)

# preprocessing
set.seed(123)
review_split <- initial_split(reviews_parsed, strata = general_rating)
review_train <- training(review_split)
review_test <- testing(review_split)

review_rec <- recipe(general_rating ~ Review, data = review_train) %>%
  step_tokenize(Review) %>%
  step_stopwords(Review) %>%
  step_tokenfilter(Review, max_tokens = 500) %>%
  step_tfidf(Review) %>%
  step_normalize(all_predictors())

review_prep <- prep(review_rec) # implements the recipe

review_prep
juice(review_prep)

# model building - selecting logistic regresssion
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lasso_wf <- workflow() %>%
  add_recipe(review_rec) %>%
  add_model(lasso_spec)
lasso_wf

# tuning parameters
lambda_grid <- grid_regular(penalty(), levels = 40)

set.seed(123)
review_folds <- bootstraps(review_train, strata = general_rating)
review_folds

set.seed(2020)
lasso_grid <- tune_grid(
  lasso_wf,
  resamples = review_folds,
  grid = lambda_grid,
  metrics = metric_set(roc_auc, ppv, npv)
)
lasso_grid

lasso_grid %>%
  collect_metrics()

lasso_grid %>% # we are looking for the peaks in all 3 variables, npv still has some left to go.
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5, legend = FALSE) +
  facet_wrap(~.metric) +
  scale_x_log10() +
  theme_jacob()

best_ppv <- lasso_grid %>%
  select_best("ppv")

best_ppv

final_lasso <- finalize_workflow(lasso_wf, best_ppv)
final_lasso

final_lasso_data <- final_lasso %>%
  fit(review_train) %>%
  pull_workflow_fit() %>%
  vi(lambda = best_ppv$penalty)

final_lasso_data %>%
  group_by(Sign) %>%
  top_n(20, wt = abs(Importance)) %>%
  ungroup() %>%
  mutate(Importance = abs(Importance),
         Variable = str_remove(Variable, "tfidf_Review_"),
         Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Sign, scales = "free_y") +
  labs(title = "LASSO Regression Results", 
       subtitle = "Ranking the Importance of Words Leading to Positive or Negative Reviews",
       x = "Importance", 
       y = "Word", 
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = "")) +
  theme_jacob()
ggsave('AmazonLASSO.png', width = 9, height = 4)

review_final <- last_fit(final_lasso, review_split)

review_final %>%
  collect_metrics()

review_final %>%
  collect_predictions() %>%
  conf_mat(general_rating, .pred_class)

conf_matrix <- review_final %>%
  collect_predictions() %>%
  conf_mat(general_rating, .pred_class)

autoplot(conf_matrix, type = "heatmap") +
  labs(title = "LASSO Regression Confusion Matrix", 
       subtitle = "79% Overall Accuracy",
       x = "Actual", 
       y = "Predicted", 
       caption = paste('Data via AWS Public Dataset Domain on ', dateVariable, sep = "")) +
  theme_jacob() +
  theme(legend.position = 'none')
ggsave('AmazonLASSOcm.png', width = 9, height = 4)

